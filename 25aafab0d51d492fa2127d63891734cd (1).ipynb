{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bc6cf59",
   "metadata": {
    "papermill": {
     "duration": 0.040325,
     "end_time": "2023-08-07T13:02:41.761843",
     "exception": false,
     "start_time": "2023-08-07T13:02:41.721518",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### Conecta no arcgis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd031cdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T13:02:41.834077Z",
     "iopub.status.busy": "2023-08-07T13:02:41.833318Z",
     "iopub.status.idle": "2023-08-07T13:02:43.333925Z",
     "shell.execute_reply": "2023-08-07T13:02:43.334363Z"
    },
    "papermill": {
     "duration": 1.54175,
     "end_time": "2023-08-07T13:02:43.334696",
     "exception": false,
     "start_time": "2023-08-07T13:02:41.792946",
     "status": "completed"
    },
    "scrolled": true,
    "tags": [
     "gis"
    ]
   },
   "outputs": [],
   "source": [
    "from arcgis.gis import GIS\n",
    "gis = GIS(\"home\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec268e6",
   "metadata": {
    "papermill": {
     "duration": 0.030525,
     "end_time": "2023-08-07T13:02:43.396624",
     "exception": false,
     "start_time": "2023-08-07T13:02:43.366099",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Atualizacao de Bases atraves de Shapefiles enviados por um formulario no sharepoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcabb66d",
   "metadata": {
    "papermill": {
     "duration": 0.030401,
     "end_time": "2023-08-07T13:02:43.458136",
     "exception": false,
     "start_time": "2023-08-07T13:02:43.427735",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 01-Entradas "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491a50b8",
   "metadata": {
    "papermill": {
     "duration": 0.034924,
     "end_time": "2023-08-07T13:02:43.523583",
     "exception": false,
     "start_time": "2023-08-07T13:02:43.488659",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "###### Conecta no Sharepoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bed699",
   "metadata": {
    "papermill": {
     "duration": 0.030394,
     "end_time": "2023-08-07T13:02:43.584571",
     "exception": false,
     "start_time": "2023-08-07T13:02:43.554177",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "########## O login e senha do sharepoint sao segredos pessoais do funcionario , portanto o creator ou qualquer administrador do arcgis nao pode deixar essas informacoes direto no codigo , ja que esse notebook pode ser compartilhado . Dessa forma a solucao e criar um csv privado para o usuario que contem o login senha do usuario. Ao mudar de Creator , nao esquecer de apagar seu login e senha do csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef8f9da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T13:02:43.654873Z",
     "iopub.status.busy": "2023-08-07T13:02:43.654386Z",
     "iopub.status.idle": "2023-08-07T13:02:43.901814Z",
     "shell.execute_reply": "2023-08-07T13:02:43.901392Z"
    },
    "papermill": {
     "duration": 0.284983,
     "end_time": "2023-08-07T13:02:43.901952",
     "exception": false,
     "start_time": "2023-08-07T13:02:43.616969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "SECRET_CSV_ITEM_ID = '44a1138231fe41779a212837023ff377'\n",
    "def get_secrets(gis=gis,\n",
    "                secret_csv_item_id=SECRET_CSV_ITEM_ID):\n",
    "    \"\"\"Returns the secrets.csv file as a dict of secret_key : secret_value\"\"\"\n",
    "    item = gis.content.get(secret_csv_item_id)\n",
    "    with open(item.download(), 'r') as local_item_file:\n",
    "        reader = csv.DictReader(local_item_file)\n",
    "        return {rows['secret_key']: rows['secret_value'] for rows in reader}\n",
    "secrets = get_secrets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35583fb0",
   "metadata": {
    "papermill": {
     "duration": 0.03072,
     "end_time": "2023-08-07T13:02:43.963613",
     "exception": false,
     "start_time": "2023-08-07T13:02:43.932893",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Ler os arquivos da lista do sharepoint e salva num diretorio local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f0c17b",
   "metadata": {
    "papermill": {
     "duration": 0.03052,
     "end_time": "2023-08-07T13:02:44.024556",
     "exception": false,
     "start_time": "2023-08-07T13:02:43.994036",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "######## Com seu usuario e senha voce pode acessar a lista  do sharepoint onde esta os pedidos do formulario . Esse codigo baixa o zip extraido depois pega todos os arquivos dentro de subpastas e coloca no diretorio principal , apaga as subpastas e deixa os shps numa pasta temp. Ai ocorrera a leitura de shp posteriormente\n",
    "E tambem o codigo verifica \"Erros crassos de arquivo\" por exemplo se o usuario envio um arquivo zipado com todos os arquivos de shapefile dentro e se esses arquivos shapefiles estao com os campos corretos. Muitas vezes campos com acentos ou caracteres comuns em ingles mas nao em pt-br sao lidos erronemeante ficando substituido pelo caracter '?' , que significa 'nao identificado' e causa problema de leitura do shapefile pela library shapefile , dessa forma e necessario que os usuarios mandem shapefiles com campos sem acentos (ou seja shapefiles criados com os moldes que eu preparei previamente)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b655f8f",
   "metadata": {
    "papermill": {
     "duration": 0.030383,
     "end_time": "2023-08-07T13:02:44.085152",
     "exception": false,
     "start_time": "2023-08-07T13:02:44.054769",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Tipos de erros:\n",
    "    Erro1: erros que comprometem o pedido inteiro ( erros de arquivo)\n",
    "    Erro2: erros que comprometem o pedido parcialmente (erros de linhas de dataframe)\n",
    "\n",
    "Toda vez que um erro novo for identificado , quando um usuario tiver um comportamento inesperado no envio , e necessario criar uma categoria ou agrega-la aos tipo de erros . Por exemplo , nesse moemnto foi dividio entre erros de arquivo e erros de dataframe . Mas pode haver outros , desta forma mantenha o rastreio de erros atualizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49ef63f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T13:02:44.152124Z",
     "iopub.status.busy": "2023-08-07T13:02:44.151396Z",
     "iopub.status.idle": "2023-08-07T13:02:44.153592Z",
     "shell.execute_reply": "2023-08-07T13:02:44.153196Z"
    },
    "papermill": {
     "duration": 0.037679,
     "end_time": "2023-08-07T13:02:44.153706",
     "exception": false,
     "start_time": "2023-08-07T13:02:44.116027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from arcgis.geometry import Polygon, Geometry, Point, Polyline\n",
    "\n",
    "def sdf_from_shp(shapefile_path):\n",
    "    sdf = pd.DataFrame.spatial.from_featureclass(shapefile_path,enconding='latin-1')\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4604ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T13:02:44.221493Z",
     "iopub.status.busy": "2023-08-07T13:02:44.220865Z",
     "iopub.status.idle": "2023-08-07T13:02:44.299829Z",
     "shell.execute_reply": "2023-08-07T13:02:44.300189Z"
    },
    "papermill": {
     "duration": 0.11673,
     "end_time": "2023-08-07T13:02:44.300396",
     "exception": false,
     "start_time": "2023-08-07T13:02:44.183666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shapefile\n",
    "from shapely.geometry import shape\n",
    "\n",
    "def filter_shapefile_latin_1(shapefile_path):\n",
    "    reader = shapefile.Reader(shapefile_path, encoding='latin-1')\n",
    "\n",
    "    filtered_records = []\n",
    "    problematic_strings = []  # Lista para armazenar as strings problemáticas\n",
    "\n",
    "    for shape_record in reader.shapeRecords():\n",
    "        record = shape_record.record\n",
    "        try:\n",
    "            # Tenta criar a geometria a partir do shape_record\n",
    "            geometry = shape(shape_record.shape.__geo_interface__)\n",
    "            filtered_records.append((record, shape_record.shape))  # Armazena o registro e a geometria\n",
    "        except UnicodeDecodeError:\n",
    "            # Armazena a string problemática na lista\n",
    "            problematic_strings.append(str(record))\n",
    "\n",
    "    # Cria um novo shapefile somente com os registros filtrados\n",
    "    writer = shapefile.Writer(shapefile_path[:-4])  # Remove a extensão .shp do caminho\n",
    "    writer.fields = reader.fields[1:]  # Copia os campos do shapefile original, excluindo o campo de exclusão\n",
    "    for record, geom in filtered_records:\n",
    "        writer.record(*record)\n",
    "        writer.shape(geom)\n",
    "\n",
    "    # Fecha o objeto Writer e salva o novo shapefile\n",
    "    writer.close()\n",
    "\n",
    "    # Retorna o vetor com as strings problemáticas\n",
    "    return problematic_strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29def5d",
   "metadata": {
    "papermill": {
     "duration": 0.030469,
     "end_time": "2023-08-07T13:02:44.361965",
     "exception": false,
     "start_time": "2023-08-07T13:02:44.331496",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "####### Como o kernel e o python constantemente nao conversam ( as vezes o kernel executa antes do python)  eu instalei os pacotes numa pasta chamada library '/arcgis/library' com o comando :pip install --target=/arcgis/library Office365-REST-Python-Client , isso deve resolver o problema dele nao encontrar os pacotes , ja que o kernel constantemente apaga os packges do diretorio de sites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb1889e",
   "metadata": {
    "papermill": {
     "duration": 0.034386,
     "end_time": "2023-08-07T13:02:44.427234",
     "exception": false,
     "start_time": "2023-08-07T13:02:44.392848",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Funcoes de offices 365 de leitura da lista do sharpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d108a61d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T13:02:44.498764Z",
     "iopub.status.busy": "2023-08-07T13:02:44.498216Z",
     "iopub.status.idle": "2023-08-07T13:02:48.273362Z",
     "shell.execute_reply": "2023-08-07T13:02:48.272570Z"
    },
    "papermill": {
     "duration": 3.811591,
     "end_time": "2023-08-07T13:02:48.273501",
     "exception": false,
     "start_time": "2023-08-07T13:02:44.461910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/arcgis/home/library\")\n",
    "from office365.runtime.auth.authentication_context import AuthenticationContext\n",
    "from office365.runtime.auth.client_credential import ClientCredential\n",
    "from office365.sharepoint.client_context import ClientContext\n",
    "from office365.sharepoint.files.file import File\n",
    "\n",
    "import zipfile\n",
    "import io \n",
    "import tempfile\n",
    "import shapefile\n",
    "#pip install Office365-REST-Python-Client\n",
    "\n",
    "sharepoint_password=secrets[\"alex.matias@pecenergia.com.br\"]\n",
    "sharepoint_base_url = 'https://pecenergia.sharepoint.com/sites/Fundiario/'\n",
    "sharepoint_user = 'alex.matias@pecenergia.com.br'\n",
    "#autentica no sharepoint e acessa as pastas\n",
    "auth = AuthenticationContext(sharepoint_base_url) \n",
    "auth.acquire_token_for_user(sharepoint_user, sharepoint_password)\n",
    "ctx = ClientContext(sharepoint_base_url, auth)\n",
    "web = ctx.web\n",
    "ctx.load(web)\n",
    "ctx.execute_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f7aaf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T13:02:48.340925Z",
     "iopub.status.busy": "2023-08-07T13:02:48.340090Z",
     "iopub.status.idle": "2023-08-07T13:02:48.342279Z",
     "shell.execute_reply": "2023-08-07T13:02:48.341844Z"
    },
    "papermill": {
     "duration": 0.038268,
     "end_time": "2023-08-07T13:02:48.342393",
     "exception": false,
     "start_time": "2023-08-07T13:02:48.304125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sharepoint_lists_fundiario(ctx,nome_lista):\n",
    "        #verifica os itens nao incluidos na base que estao na lista\n",
    "        \"\"\"\n",
    "        itens incluidos tem seu Title modificado para 'Incluido_na_base(editado_power_automate)' quando estao sem erro\n",
    "        Quando estao com algum Erro de formato nao esperado de anexo , ou seja nao e um arquivo .zip com shapefiles dentro\n",
    "        nomea-se \"Erro1\" e \"Erro2\" para erro nos campos do shapefile\n",
    "        \"\"\"\n",
    "        titulos_esperados=['Incluido_na_base(editado_power_automate)','Erro1','Erro2']\n",
    "        #Retorna a lista do sharepoint\n",
    "        sp_site_lists = ctx.web.lists.get_by_title(nome_lista)\n",
    "        all_items = sp_site_lists.items.paged(1000).get().execute_query()\n",
    "        list_items = [item for item in all_items if item.properties[\"Title\"] not in titulos_esperados]\n",
    "        return list_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e46724",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T13:02:48.411323Z",
     "iopub.status.busy": "2023-08-07T13:02:48.410531Z",
     "iopub.status.idle": "2023-08-07T13:02:48.412825Z",
     "shell.execute_reply": "2023-08-07T13:02:48.412427Z"
    },
    "papermill": {
     "duration": 0.038164,
     "end_time": "2023-08-07T13:02:48.412942",
     "exception": false,
     "start_time": "2023-08-07T13:02:48.374778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def erro1(list_item):\n",
    "    list_item.set_property('Title', \"Erro1\")\n",
    "    erros=['Erro de leitura de arquivo pode ser que nao e um arquivo .zip ou nao contem os arquivos necessarios para o .shp ou seja .shp, .shx e .dbf',\n",
    "           'O pedido deve conter um anexo e apenas um anexo'\n",
    "                    ,'Outro motivo e que pode nao estar no padrao utf-8 , geralmente e causado por conter acentos ou cedilha nos campos de coluna ou campos de escrita'\n",
    "                    ,'tente remover os acentos se for o caso e tente novamente , padrao ingles nao contem acentos']\n",
    "    string_erros='\\n'.join(erro for erro in erros)\n",
    "    list_item.set_property('Tipodeerro', string_erros)\n",
    "    list_item.update()\n",
    "    ctx.execute_query()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fac5a94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T13:02:48.484876Z",
     "iopub.status.busy": "2023-08-07T13:02:48.484065Z",
     "iopub.status.idle": "2023-08-07T13:02:48.485901Z",
     "shell.execute_reply": "2023-08-07T13:02:48.486196Z"
    },
    "papermill": {
     "duration": 0.043183,
     "end_time": "2023-08-07T13:02:48.486345",
     "exception": false,
     "start_time": "2023-08-07T13:02:48.443162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def download_attachments(ctx, list_item):\n",
    "    # Obter anexos do item da lista\n",
    "    attachments = list_item.attachment_files.get().execute_query()\n",
    "    # Verificar se o título do anexo é um arquivo zip com shapefiles dentro\n",
    "    if len(attachments)!=1:\n",
    "        #erro1(list_item)\n",
    "        print(\"Arquivo .zip removidos do pedido de operacao na base de ID\",list_item.properties[\"ID\"], \"esta incorreto\") \n",
    "        return ('erro1','erro1')\n",
    "    else:\n",
    "        attachment_title = attachments[0].properties[\"FileName\"]\n",
    "        if not attachment_title.endswith(\".zip\"):\n",
    "            #erro1(list_item)\n",
    "            return ('erro1','erro1')\n",
    "        else:\n",
    "            # Obter informações do item da lista\n",
    "            projeto = list_item.properties['Projeto']\n",
    "            operacao = list_item.properties['TipodeOperacaonabase']\n",
    "            item_id = list_item.properties[\"Id\"]\n",
    "            creator_id=list_item.properties[\"AuthorId\"]\n",
    "\n",
    "            if(list_item.properties[\"Attachments\"]):\n",
    "                # Baixar cada anexo para o diretório correspondente\n",
    "                for attachment in attachments:\n",
    "                    file_name = f\"{projeto}_{operacao}_{creator_id}_{item_id}.zip\"\n",
    "                    folder_path = f\"/arcgis/home/Nao Incluidos/{operacao}\"\n",
    "                    caminho=os.path.join(folder_path, file_name)\n",
    "\n",
    "                    # Baixa o conteúdo do anexo e o escreve em um arquivo\n",
    "                    with open(caminho, \"wb\") as f:\n",
    "                        content=attachment.download(f).execute_query()\n",
    "                    print(verificar_arquivo_shapefile_zip(caminho))\n",
    "                    if(verificar_arquivo_shapefile_zip(caminho)):\n",
    "                        with zipfile.ZipFile(caminho) as zip_ref:    \n",
    "                            zip_ref.extractall('/arcgis/home/Nao Incluidos/temp')    \n",
    "                        cria_zip_legivel(folder_path, file_name)\n",
    "                        print(\"Download de anexo com sucesso\")\n",
    "                    else : \n",
    "                        print(\"nao e arquivo .zip ou nao contem extensoes shapefile dentro\")\n",
    "                        operacao='erro1'\n",
    "                os.remove(os.path.join(folder_path, file_name[:-4]+'.zip'))        \n",
    "                return (os.path.join(folder_path, file_name[:-4]+'.shp'),operacao)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5191fd",
   "metadata": {
    "papermill": {
     "duration": 0.030424,
     "end_time": "2023-08-07T13:02:48.547083",
     "exception": false,
     "start_time": "2023-08-07T13:02:48.516659",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Funcoes que verificam o .zip e colocam os arquivos num formato esperado ( um arquivo zipado com os arquivos .shp dentro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc0a644",
   "metadata": {
    "papermill": {
     "duration": 0.030589,
     "end_time": "2023-08-07T13:02:48.608024",
     "exception": false,
     "start_time": "2023-08-07T13:02:48.577435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3b80183",
   "metadata": {
    "papermill": {
     "duration": 0.037629,
     "end_time": "2023-08-07T13:02:48.676702",
     "exception": false,
     "start_time": "2023-08-07T13:02:48.639073",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Verifica se o .shp esta no formato correto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe4e449",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T13:02:48.744694Z",
     "iopub.status.busy": "2023-08-07T13:02:48.743888Z",
     "iopub.status.idle": "2023-08-07T13:02:48.745836Z",
     "shell.execute_reply": "2023-08-07T13:02:48.746191Z"
    },
    "papermill": {
     "duration": 0.039317,
     "end_time": "2023-08-07T13:02:48.746350",
     "exception": false,
     "start_time": "2023-08-07T13:02:48.707033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CONSERTAR COM OS MOLDES CORRETOS    \n",
    "def verifica_campos_shp(shapefile_path,tipo):\n",
    "    #SO FUNCIONA PRO MOLDE SEM ACENTO\n",
    "    print(shapefile_path)\n",
    "    try :\n",
    "        sdf = sdf_from_shp(shapefile_path)\n",
    "    except:\n",
    "        strings_com_erro=filter_shapefile_latin_1(shapefile_path)\n",
    "        print(strings_com_erro)\n",
    "        try: sdf = sdf_from_shp(shapefile_path)\n",
    "        except:\n",
    "            print(\"erro na codificacao , nao e latin-1\")\n",
    "            return False\n",
    "    display(sdf)\n",
    "    if(tipo=='Inclusao'):\n",
    "        lista_campos=[\"Status\",\"Proprieta\",\"Matricula\",\"Contrato\",\"Imovel\"]\n",
    "        colunas=[\"Status\",\"Proprieta\"]\n",
    "    if(tipo=='Edicao'):\n",
    "        lista_campos=['area_code']\n",
    "        colunas=['area_code']\n",
    "    if(tipo=='Divisao'):\n",
    "        lista_campos=['area_code',\"Status\",\"Proprieta\",\"Matricula\",\"Contrato\",\"Imovel\"]\n",
    "        colunas=[\"Status\",\"Proprieta\"]\n",
    "    \n",
    "    if set(lista_campos).issubset(set(sdf.columns)):\n",
    "        aceito=True\n",
    "    else:\n",
    "        aceito=False\n",
    "\n",
    "    return aceito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf8d986",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T13:02:48.817266Z",
     "iopub.status.busy": "2023-08-07T13:02:48.816562Z",
     "iopub.status.idle": "2023-08-07T13:02:48.821407Z",
     "shell.execute_reply": "2023-08-07T13:02:48.821040Z"
    },
    "papermill": {
     "duration": 0.043653,
     "end_time": "2023-08-07T13:02:48.821523",
     "exception": false,
     "start_time": "2023-08-07T13:02:48.777870",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def cria_zip_legivel(roots, file_name):\n",
    "    import zipfile\n",
    "    import os\n",
    "    import shutil\n",
    "    # Define o nome do arquivo zip original e do arquivo zip novo\n",
    "    file_name=file_name[:-4]\n",
    "    # Percorre as subpastas da pasta extraída e move os arquivos para a pasta principal\n",
    "    for root, dirs, files in os.walk('/arcgis/home/Nao Incluidos/temp'):\n",
    "        for file in files:\n",
    "            if(not(file.endswith('.zip'))):\n",
    "                os.rename(os.path.join(root, file), os.path.join(root, file_name+file[-4:]))\n",
    "                shutil.move(os.path.join(root, file_name+file[-4:]), os.path.join(roots, file_name+file[-4:]))\n",
    "                \n",
    "    caminho = '/arcgis/home/Nao Incluidos/temp'\n",
    "    for root, dirs, files in os.walk(caminho):\n",
    "            for dir in dirs:\n",
    "                shutil.rmtree(os.path.join(root, dir))\n",
    "\n",
    "def verificar_arquivo_shapefile_zip(nome_arquivo_zip):\n",
    "    import zipfile\n",
    "\n",
    "    # Lista de extensões esperadas\n",
    "    extensoes_esperadas = ['shp', 'shx', 'dbf']  # Colocar extensao sem '.'\n",
    "\n",
    "    # Dicionário para contar o número de ocorrências de cada extensão\n",
    "    ocorrencias_extensoes = {extensao: 0 for extensao in extensoes_esperadas}\n",
    "\n",
    "    # Função para verificar se um nome de arquivo corresponde a uma extensão esperada\n",
    "    def verificar_extensao(nome_arquivo):\n",
    "        extensao = nome_arquivo.split('.')[-1].lower()\n",
    "        return extensao in extensoes_esperadas\n",
    "\n",
    "    # Função para percorrer todas as pastas e subpastas dentro do arquivo zip\n",
    "    def percorrer_zip(zip, caminho='', ocorrencias_extensoes=None):\n",
    "        if ocorrencias_extensoes is None:\n",
    "            ocorrencias_extensoes = {}\n",
    "        for nome_arquivo in zip.namelist():\n",
    "            partes_caminho = nome_arquivo.split('/')\n",
    "            if nome_arquivo.startswith(caminho) and verificar_extensao(nome_arquivo):\n",
    "                extensao = nome_arquivo.split('.')[-1].lower()\n",
    "                ocorrencias_extensoes[extensao] = ocorrencias_extensoes.get(extensao, 0) + 1\n",
    "        return ocorrencias_extensoes\n",
    "\n",
    "    # Abrir o arquivo zip\n",
    "    with zipfile.ZipFile(nome_arquivo_zip, 'r') as zip:\n",
    "        # Percorrer todas as pastas e subpastas dentro do arquivo zip\n",
    "        ocorrencias_extensoes = percorrer_zip(zip, ocorrencias_extensoes=ocorrencias_extensoes)\n",
    "    \n",
    "    print(ocorrencias_extensoes)\n",
    "\n",
    "    # Verificar se cada extensão ocorreu apenas uma vez\n",
    "    for extensao, ocorrencias in ocorrencias_extensoes.items():\n",
    "        if ocorrencias != 1:\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def error_report(arquivo_zip, tipo):\n",
    "    if verificar_arquivo_shapefile_zip(arquivo_zip):\n",
    "        print(\"É um arquivo zip\")\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453d0276",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T13:02:48.892007Z",
     "iopub.status.busy": "2023-08-07T13:02:48.891473Z",
     "iopub.status.idle": "2023-08-07T13:03:00.339953Z",
     "shell.execute_reply": "2023-08-07T13:03:00.340308Z"
    },
    "papermill": {
     "duration": 11.488204,
     "end_time": "2023-08-07T13:03:00.340473",
     "exception": false,
     "start_time": "2023-08-07T13:02:48.852269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    print('Conectado ao SharePoint: ',web.properties['Title'])\n",
    "    items=sharepoint_lists_fundiario(ctx,'Solicitar Modificacao de base')\n",
    "    exclusoes=pd.DataFrame(columns=[\"ID_pedido\", \"Projeto\",\"Exclusao\"])\n",
    "    for item in items:\n",
    "        if(item.properties[\"TipodeOperacaonabase\"]!='Exclusao'):\n",
    "            print(item.properties[\"ID\"])\n",
    "            (caminho,tipo)=download_attachments(ctx,item)\n",
    "            if(tipo!='erro1'):\n",
    "                if(not(verifica_campos_shp(caminho,tipo))):\n",
    "                    print(\"nao aceita\")\n",
    "                    erro1(item)\n",
    "                    nome_arquivo = os.path.basename(caminho)\n",
    "                    diretorio = os.path.dirname(caminho)\n",
    "                    list_ext=['.dbf','.prj','.shp','.shx','cpg']\n",
    "                    for ext in list_ext:\n",
    "                        path = os.path.join(diretorio, nome_arquivo[:-4]+ext)\n",
    "                        if (os.path.exists(path)):\n",
    "                            os.remove(path)\n",
    "\n",
    "                    print(\"Arquivo .zip removidos do pedido de operacao na base de ID\",item.properties[\"ID\"], \"esta incorreto\")            \n",
    "\n",
    "\n",
    "                else: print(\"Arquivo .zip :\",os.path.basename(caminho),\"do pedido de operacao na base de ID\",item.properties[\"ID\"], \"esta correto\") \n",
    "\n",
    "            else:erro1(item)\n",
    "main()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b2faa5",
   "metadata": {
    "papermill": {
     "duration": 0.035347,
     "end_time": "2023-08-07T13:03:00.412297",
     "exception": false,
     "start_time": "2023-08-07T13:03:00.376950",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 02-Operacoes do algoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d6f73b",
   "metadata": {
    "papermill": {
     "duration": 0.035805,
     "end_time": "2023-08-07T13:03:00.484504",
     "exception": false,
     "start_time": "2023-08-07T13:03:00.448699",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Transfroma os shapefiles num conjunto de sdf (spartial dataframe) , depois compara para ver se o sdf intersecta com o conjunto de poligonos de todos os projetos salvando os que foram rejeitados para mandar por email para seus donos \n",
    "#### verifica tambem se contem conteudo correto nos campo dos moldes e se esta correto e recusa caso nao esteja"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3159881a",
   "metadata": {
    "papermill": {
     "duration": 0.035903,
     "end_time": "2023-08-07T13:03:00.556211",
     "exception": false,
     "start_time": "2023-08-07T13:03:00.520308",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "###### Verifica os shps e atualiza a base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af6c19f",
   "metadata": {
    "papermill": {
     "duration": 0.035967,
     "end_time": "2023-08-07T13:03:00.628006",
     "exception": false,
     "start_time": "2023-08-07T13:03:00.592039",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Le os shps , verifica as insterseccoes e retorna os features recusados "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d27780",
   "metadata": {
    "papermill": {
     "duration": 0.035978,
     "end_time": "2023-08-07T13:03:00.700015",
     "exception": false,
     "start_time": "2023-08-07T13:03:00.664037",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Funcoes de content manegement do arcgis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e018d0b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T13:03:00.776774Z",
     "iopub.status.busy": "2023-08-07T13:03:00.775979Z",
     "iopub.status.idle": "2023-08-07T13:03:00.777659Z",
     "shell.execute_reply": "2023-08-07T13:03:00.777953Z"
    },
    "papermill": {
     "duration": 0.042302,
     "end_time": "2023-08-07T13:03:00.778103",
     "exception": false,
     "start_time": "2023-08-07T13:03:00.735801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "projetos =[]#nome dos projetos nos mapas do arcgis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4956dd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T13:03:00.855312Z",
     "iopub.status.busy": "2023-08-07T13:03:00.854514Z",
     "iopub.status.idle": "2023-08-07T13:03:00.855996Z",
     "shell.execute_reply": "2023-08-07T13:03:00.856433Z"
    },
    "papermill": {
     "duration": 0.042847,
     "end_time": "2023-08-07T13:03:00.856586",
     "exception": false,
     "start_time": "2023-08-07T13:03:00.813739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retorna_posicao(lista,elemento):\n",
    "    \"\"\"\n",
    "    Retorna a posicao de um elemento em uma lista de objetos\n",
    "    \"\"\"\n",
    "    for i in range(len(lista)):\n",
    "        mapa=str(lista[i])\n",
    "        split=mapa.replace('\"','').replace(' type','').split(':')[1]\n",
    "        if(split==elemento):\n",
    "            return i "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ef3a97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T13:03:00.935381Z",
     "iopub.status.busy": "2023-08-07T13:03:00.934649Z",
     "iopub.status.idle": "2023-08-07T13:03:00.936829Z",
     "shell.execute_reply": "2023-08-07T13:03:00.936458Z"
    },
    "papermill": {
     "duration": 0.044654,
     "end_time": "2023-08-07T13:03:00.936943",
     "exception": false,
     "start_time": "2023-08-07T13:03:00.892289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from arcgis.geometry import lengths, areas_and_lengths, project\n",
    "import pandas as pd\n",
    "def reproject_sdf(sdf,epsg):\n",
    "    if(epsg==3857):\n",
    "        out_SR={'wkid': 102100, 'latestWkid': 3857}\n",
    "    if(epsg==4326):\n",
    "        out_SR={'wkid': 4326}\n",
    "    result = project(\n",
    "        geometries=list(sdf['SHAPE']),\n",
    "        in_sr=sdf.spatial.sr,\n",
    "        out_sr=out_SR\n",
    "    )\n",
    "\n",
    "    sdf_reprojetado = pd.DataFrame(columns=sdf.columns)\n",
    "\n",
    "    for col in list(sdf):\n",
    "        if col != \"SHAPE\":\n",
    "            sdf_reprojetado.loc[:, col] = sdf[col].values\n",
    "\n",
    "    for i, row in sdf_reprojetado.iterrows():\n",
    "        sdf_reprojetado.at[i, 'SHAPE']  = result[i]\n",
    "        \n",
    "    # Define a coluna \"SHAPE\" como coluna espacial do DataFrame\n",
    "    sdf_reprojetado.spatial.set_geometry('SHAPE', inplace=True)\n",
    "    return sdf_reprojetado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38033de8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T13:03:01.015269Z",
     "iopub.status.busy": "2023-08-07T13:03:01.014486Z",
     "iopub.status.idle": "2023-08-07T13:03:01.016147Z",
     "shell.execute_reply": "2023-08-07T13:03:01.016536Z"
    },
    "papermill": {
     "duration": 0.043788,
     "end_time": "2023-08-07T13:03:01.016695",
     "exception": false,
     "start_time": "2023-08-07T13:03:00.972907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retorna_df_areas_projetos(projetos):\n",
    "    dict_layer = {}\n",
    "    for projeto in projetos:\n",
    "        map_projeto = gis.content.search('title:' + projeto, 'Feature Layer')\n",
    "        Projeto_item = map_projeto[retorna_posicao(map_projeto, projeto)]\n",
    "        Projeto_layers = Projeto_item.layers\n",
    "        Projeto_tables = Projeto_item.tables\n",
    "        dict_layer[projeto]={}\n",
    "        for layer in Projeto_layers:\n",
    "            lyr_fset = layer.query()\n",
    "            lyr_sdf = lyr_fset.sdf\n",
    "            dict_layer[projeto][layer.properties.name]={'sublayer': layer, 'sdf': lyr_sdf}\n",
    "    return dict_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5f95ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T13:03:01.094853Z",
     "iopub.status.busy": "2023-08-07T13:03:01.094103Z",
     "iopub.status.idle": "2023-08-07T13:03:01.096290Z",
     "shell.execute_reply": "2023-08-07T13:03:01.095877Z"
    },
    "papermill": {
     "duration": 0.044514,
     "end_time": "2023-08-07T13:03:01.096415",
     "exception": false,
     "start_time": "2023-08-07T13:03:01.051901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def efetua_exclusoes(sdf,sublayer,operacao_exclusao,Projeto):\n",
    "    print(operacao_exclusao)\n",
    "    if(not(operacao_exclusao)):\n",
    "        dict_layers=retorna_df_areas_projetos([Projeto])\n",
    "        sdf_sublayer=dict_layers[Projeto][sublayer]['sdf']\n",
    "        if not sdf_sublayer.empty:\n",
    "            lista_area_code = sdf_sublayer['area_code'].unique()\n",
    "            condicao = sdf['area_code'].isin(lista_area_code)\n",
    "            display(sdf_sublayer[condicao][['OBJECTID']])\n",
    "            lista_deletes=[]\n",
    "            lista_deletes = str(sdf_sublayer[condicao][['OBJECTID']].tolist())\n",
    "            print(lista_deletes)\n",
    "            if(lista_deletes!=[]):result=dict_layers[Projeto][sublayer]['sublayer'].edit_features(deletes=lista)\n",
    "            if(result):\n",
    "                print(\"para o sublayer\", sublayer,\"temos:\")\n",
    "                print(\"features deletados de area_code\",areas_code)\n",
    "                if(operacao_exclusao):confirma_pedidos(sdf,'Exclusao')\n",
    "    else : print(\"nao entro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3d64e9",
   "metadata": {
    "papermill": {
     "duration": 0.035224,
     "end_time": "2023-08-07T13:03:01.167103",
     "exception": false,
     "start_time": "2023-08-07T13:03:01.131879",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Funcoes de verificacoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559450d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T13:03:01.242910Z",
     "iopub.status.busy": "2023-08-07T13:03:01.242252Z",
     "iopub.status.idle": "2023-08-07T13:03:01.244561Z",
     "shell.execute_reply": "2023-08-07T13:03:01.244154Z"
    },
    "papermill": {
     "duration": 0.042257,
     "end_time": "2023-08-07T13:03:01.244689",
     "exception": false,
     "start_time": "2023-08-07T13:03:01.202432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "projetos_GPD =[]#projetos com calculo GPD\n",
    "projetos_novos=[]#projetos novos sem interseccao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39f38d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T13:03:01.390408Z",
     "iopub.status.busy": "2023-08-07T13:03:01.389673Z",
     "iopub.status.idle": "2023-08-07T13:03:01.391797Z",
     "shell.execute_reply": "2023-08-07T13:03:01.391442Z"
    },
    "papermill": {
     "duration": 0.042005,
     "end_time": "2023-08-07T13:03:01.391911",
     "exception": false,
     "start_time": "2023-08-07T13:03:01.349906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def atribuir_erro(sdf,erro_tipo,condicao):\n",
    "    sdf.loc[condicao, 'erro'] = erro_tipo\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ac57e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T13:03:01.473114Z",
     "iopub.status.busy": "2023-08-07T13:03:01.472322Z",
     "iopub.status.idle": "2023-08-07T13:03:01.474043Z",
     "shell.execute_reply": "2023-08-07T13:03:01.474339Z"
    },
    "papermill": {
     "duration": 0.047049,
     "end_time": "2023-08-07T13:03:01.474491",
     "exception": false,
     "start_time": "2023-08-07T13:03:01.427442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def verifica_campos_vazios(sdf, tipo, filename):\n",
    "    Projeto = filename.split('_')[0]\n",
    "    print(Projeto)\n",
    "    dict_layers=retorna_df_areas_projetos([Projeto])\n",
    "    sdf_props = dict_layers[Projeto]['Areas']['sdf']\n",
    "    ID_pedido = filename.split('_')[3][:-4]\n",
    "    \n",
    "    sdf['ID_pedido'] = [ID_pedido] * len(sdf)  # Rastreio do pedido\n",
    "    sdf[\"erro\"] = np.nan * len(sdf)\n",
    "\n",
    "    # Verifica erros de geometria\n",
    "    condicao = ~(sdf['SHAPE'].apply(lambda geom: geom.is_valid() if geom is not None else False))\n",
    "    sdf = atribuir_erro(sdf, 8, condicao)\n",
    "    # Bloco case\n",
    "    if(tipo=='Edicao'):\n",
    "        actions = {\n",
    "        'Edicao': [\n",
    "                    (3, ~sdf['area_code'].isin(sdf.drop_duplicates(subset=['area_code'], keep=False)['area_code'])),\n",
    "                    (2, ~sdf['area_code'].isin(sdf_props['area_code'].unique()))\n",
    "                    ]\n",
    "                    }\n",
    "    if(tipo=='Inclusao'):\n",
    "        actions = {\n",
    "            'Inclusao': [\n",
    "                (5,  ~(((sdf['Matricula'].notnull() | sdf['Contrato'].notnull()) & (sdf['Status'] == 'Mapeado com documentos')) | (sdf['Status'] == 'Mapeado sem documentos')) & (sdf['Status'].notnull() & sdf['Proprieta'].notnull()))\n",
    "                ]\n",
    "            }\n",
    "    if tipo in actions:\n",
    "        for error_code, condition in actions[tipo]:\n",
    "            sdf = atribuir_erro(sdf, error_code, condition)\n",
    "\n",
    "    # Reprojeta apenas os sdf que não contêm erros\n",
    "    condicao_erro_nan = sdf['erro'].isna()\n",
    "    if not sdf[condicao_erro_nan].empty:\n",
    "        reprojected_df = reproject_sdf(sdf[condicao_erro_nan], 3857)\n",
    "        sdf.loc[condicao_erro_nan, 'SHAPE'] = reprojected_df.set_index(sdf[condicao_erro_nan].index)['SHAPE']\n",
    "\n",
    "    \n",
    "\n",
    "    return sdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc9e4d3",
   "metadata": {
    "papermill": {
     "duration": 0.036049,
     "end_time": "2023-08-07T13:03:01.546606",
     "exception": false,
     "start_time": "2023-08-07T13:03:01.510557",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Funcoes de atualizacoes nos mapas & sharepoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b376063",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T13:03:01.626300Z",
     "iopub.status.busy": "2023-08-07T13:03:01.625204Z",
     "iopub.status.idle": "2023-08-07T13:03:01.626944Z",
     "shell.execute_reply": "2023-08-07T13:03:01.627266Z"
    },
    "papermill": {
     "duration": 0.044897,
     "end_time": "2023-08-07T13:03:01.627420",
     "exception": false,
     "start_time": "2023-08-07T13:03:01.582523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "def zipar_arquivos(nome_arquivo_zip,pasta):\n",
    "    arquivos_zip = [arquivo for arquivo in os.listdir(pasta) if arquivo.startswith(\"recusada_\") and arquivo.endswith((\".shp\", \".shx\", \".prj\",\"dbf\"))]\n",
    "    \n",
    "    with zipfile.ZipFile(os.path.join(pasta, nome_arquivo_zip), 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        [zipf.write(os.path.join(pasta, arquivo), os.path.relpath(os.path.join(pasta, arquivo), pasta)) for arquivo in arquivos_zip]\n",
    "    \n",
    "    for arquivo in arquivos_zip:\n",
    "        caminho_completo = os.path.join(pasta, arquivo)\n",
    "        os.remove(caminho_completo)\n",
    "    \n",
    "    return os.path.join(pasta, nome_arquivo_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a373e7db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T13:03:01.704863Z",
     "iopub.status.busy": "2023-08-07T13:03:01.704088Z",
     "iopub.status.idle": "2023-08-07T13:03:01.705743Z",
     "shell.execute_reply": "2023-08-07T13:03:01.706069Z"
    },
    "papermill": {
     "duration": 0.042223,
     "end_time": "2023-08-07T13:03:01.706220",
     "exception": false,
     "start_time": "2023-08-07T13:03:01.663997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retorna_area(geometria):\n",
    "    return geometria.area/4046.85642"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eeee08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T13:03:01.790271Z",
     "iopub.status.busy": "2023-08-07T13:03:01.789441Z",
     "iopub.status.idle": "2023-08-07T13:03:01.791231Z",
     "shell.execute_reply": "2023-08-07T13:03:01.791534Z"
    },
    "papermill": {
     "duration": 0.049365,
     "end_time": "2023-08-07T13:03:01.791693",
     "exception": false,
     "start_time": "2023-08-07T13:03:01.742328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from arcgis.features import FeatureLayer\n",
    "from arcgis.geometry.functions import *\n",
    "from arcgis.geometry import Polygon, Geometry, Point, Polyline\n",
    "from arcgis.geocoding import geocode\n",
    "from arcgis.geometry.filters import *\n",
    "\n",
    "def encontra_sobreposicao(sdf1, sdf2, col1, col2):\n",
    "    # converter geometrias do SDF em objetos Geometry\n",
    "    geometries1 = [Geometry(geometry) for geometry in sdf1['SHAPE']]\n",
    "    geometries2 = [Geometry(geometry) for geometry in sdf2['SHAPE']]\n",
    "    \n",
    "    # cria objeto R-tree a partir do segundo SpatialDataFrame\n",
    "    rtree = sdf2.spatial.sindex('rtree', reset=False)\n",
    "    # encontra índices das geometrias de sdf2 que se intersectam com sdf1\n",
    "    intersecting_indexes = set()\n",
    "    for i, geometry in enumerate(geometries1):\n",
    "        intersecting_indexes.update(rtree.intersect(geometry.extent))\n",
    "    \n",
    "    # converte índices em pares únicos de geometrias que se intersectam\n",
    "    intersecting_pairs = set()\n",
    "    for i in intersecting_indexes:\n",
    "        if i < len(sdf2):\n",
    "            # a geometria é do segundo sdf\n",
    "            for j, geometry in enumerate(geometries1):\n",
    "                try:\n",
    "                    if Geometry(geometry).intersect(Geometry(sdf2.loc[i, 'SHAPE'])):\n",
    "                        intersecting_pairs.add((j, i))\n",
    "\n",
    "                except:\n",
    "                    print(\"Para a propriedade OBJECTID\", sdf1[col1][j], \"foi detectado um erro de topologia na interseção\")\n",
    "                    sdf1.iloc[j, sdf1.columns.get_loc(\"erro\")] = 8\n",
    "                    \n",
    "\n",
    "    # criar um dicionário para armazenar as áreas das interseções\n",
    "    areas_intersect = pd.DataFrame(columns=[col1, \"area_acres\", col2, \"SHAPE\"])\n",
    "    \n",
    "    # calcular as áreas das interseções e armazená-las no dicionário\n",
    "    for i, j in intersecting_pairs:\n",
    "        try:\n",
    "            intersection = geometries1[i].intersect(geometries2[j])\n",
    "            \n",
    "            if not intersection.is_empty and 'rings' in intersection:\n",
    "                areas_intersect = areas_intersect.append({ \n",
    "                                            \"area_acres\": abs(retorna_area(intersection)),\n",
    "                                            col1: sdf1[col1][i],\n",
    "                                            col2: sdf2[col2][j],\n",
    "                                            \"SHAPE\": intersection\n",
    "                                        }, ignore_index=True)\n",
    "        except Exception as e:\n",
    "            # Tratar casos de erro de Topologia, ou seja, erros de ordem de pontos (8)\n",
    "            if 'TopologyException' and 'geom 1' in str(e):\n",
    "                print(\"Para a propriedade OBJECTID\", sdf1[col1][i], \"foi detectado um erro de topologia na interseção\")\n",
    "                sdf1.iloc[i, sdf1.columns.get_loc(\"erro\")] = 8\n",
    "\n",
    "    areas_intersect.spatial.set_geometry('SHAPE', inplace=True)\n",
    "    return areas_intersect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee5bd49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T13:03:01.869821Z",
     "iopub.status.busy": "2023-08-07T13:03:01.869045Z",
     "iopub.status.idle": "2023-08-07T13:03:01.870643Z",
     "shell.execute_reply": "2023-08-07T13:03:01.870964Z"
    },
    "papermill": {
     "duration": 0.043369,
     "end_time": "2023-08-07T13:03:01.871143",
     "exception": false,
     "start_time": "2023-08-07T13:03:01.827774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dict_erros = { 1:\"O area_code solicitado na Edição ou Divisão não está no padrão exigido, ou seja, PROP-XXX-YYYY\",\n",
    "                2:\"O area_code solicitado na Edição ou Divisão , não existe na base (*)\",\n",
    "                3:\"o area_code esta duplicado dentro do arquivo\",\n",
    "                4:\"Campos vazios em Proprietar e Status para inclusao\",\n",
    "                5:\"Status é invalido\",\n",
    "                6:\"Status é 'Mapeado com documentos' com campos de Matricula ou Contrato vazios\",\n",
    "                7:\"Existe uma interseccao muito alta(maior que 2 ha) e portanto essa prop deve ou receber um pedido especial ou ser revista .Em anexo um shapefile que contem  a prop que esta intersectando a prop rejeitada e o tamanho dessa inteseccao em (ha)\",\n",
    "                8:\"Geometria incorreta: e necessario sempre desenhar geometrias corretas nas props , exemplos de geometrias incorretas: poliigonos que contem um buraco , poligonos que estao divididos ao meio no mesmo item (por exemplo numa linha do shp voce colocou duas props , esta incorreto). Erros de geometria impedem a execucao de operacoes geograficas ,tais como interseccao , uniao ...   favor verificar a geometria.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa4293b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T13:03:01.953901Z",
     "iopub.status.busy": "2023-08-07T13:03:01.952212Z",
     "iopub.status.idle": "2023-08-07T13:03:01.955680Z",
     "shell.execute_reply": "2023-08-07T13:03:01.956015Z"
    },
    "papermill": {
     "duration": 0.048626,
     "end_time": "2023-08-07T13:03:01.956168",
     "exception": false,
     "start_time": "2023-08-07T13:03:01.907542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def confirma_pedidos(sdf,tipo_operacao):\n",
    "    caminho = '/arcgis/home/Nao Incluidos'\n",
    "    items = sharepoint_lists_fundiario(ctx, 'Solicitar Modificacao de base')\n",
    "    ID = sdf['ID_pedido'].drop_duplicates().iloc[0]\n",
    "    for item in items:\n",
    "        if str(item.properties['ID']) == str(ID):\n",
    "            if sdf['erro'].notnull().any():\n",
    "                item.set_property('Title', 'Erro2')\n",
    "                erros = sdf['erro'].dropna().drop_duplicates().tolist()\n",
    "                string_erros = '\\n'.join(dict_erros[erro] for erro in erros)\n",
    "                item.set_property('Tipodeerro', string_erros)\n",
    "                item.update()\n",
    "                ctx.execute_query()\n",
    "                path = os.path.join(caminho, 'temp', 'recusada_' + ID)\n",
    "                colunas_erro=['erro','Proprietario_principal','SHAPE']\n",
    "                if 'area_acres_intersect' and 'area_code_intersect' in sdf.columns:\n",
    "                    colunas_erro.append('area_acres_intersect')\n",
    "                    colunas_erro.append('area_code_intersect')\n",
    "        \n",
    "                sdf[~sdf['erro'].isna()][colunas_erro].spatial.to_featureclass(location=path, overwrite=True)\n",
    "                arquivo_zip = zipar_arquivos(os.path.join(caminho, 'temp',ID + '_' + 'Erro2.zip'),os.path.join(caminho, 'temp'))\n",
    "\n",
    "                with open(arquivo_zip, 'rb') as fh:\n",
    "                    file_content = fh.read()\n",
    "                \n",
    "                print(arquivo_zip)\n",
    "                folder_in_sharepoint = '/sites/Fundiario/Bases%20Cartogrficas/00 -Historico de Alteracoes - Base Arcgis/Incluidos/Erros'\n",
    "                name = os.path.basename(arquivo_zip)\n",
    "                target_folder = ctx.web.get_folder_by_server_relative_url(folder_in_sharepoint)\n",
    "                target_file = target_folder.upload_file(name, file_content).execute_query()\n",
    "                \n",
    "                os.remove(arquivo_zip)\n",
    "            else:\n",
    "                item.set_property('Title', 'Incluido_na_base(editado_power_automate)')\n",
    "                item.update()\n",
    "                ctx.execute_query()\n",
    "\n",
    "            if tipo_operacao != 'Exclusao':\n",
    "                for root, dirs, files in os.walk(caminho):\n",
    "                    for file in files:\n",
    "                        if ID in file:\n",
    "                            os.remove(os.path.join(root, file))\n",
    "                    print(\"objeto de ID\", ID, \"apagado\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f846456",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T13:03:02.043049Z",
     "iopub.status.busy": "2023-08-07T13:03:02.042208Z",
     "iopub.status.idle": "2023-08-07T13:03:02.046470Z",
     "shell.execute_reply": "2023-08-07T13:03:02.046084Z"
    },
    "papermill": {
     "duration": 0.054247,
     "end_time": "2023-08-07T13:03:02.046587",
     "exception": false,
     "start_time": "2023-08-07T13:03:01.992340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def edit_features_sdf(sdf_edits,tipo,layer_projeto,Projeto):\n",
    "        \"\"\"\n",
    "            No caso das edicoes de Interseccoes e areas aprovadas podem ocorrer de ter mais de um caso para cada prop , \n",
    "            dessa forma para garantir que as edicoes nao deixem features \"a mais\" e necessario primeiro apagar e entao adicionar\n",
    "        \"\"\" \n",
    "        actions = {\n",
    "                    'Edicao': {\n",
    "                        'Areas_Intersect': ('adds', False),\n",
    "                        'Areas_Aprovadas': ('adds', False),\n",
    "                        'Areas': ('updates', None)\n",
    "                    },\n",
    "                    'Inclusao': {\n",
    "                        'Areas_Intersect': ('adds', True),\n",
    "                        'Areas_Aprovadas': ('adds', True),\n",
    "                        'Areas': ('adds', True)\n",
    "                    }\n",
    "                }\n",
    "        #Agora e necessario adicionar nos mapas e marcar o pedido como aceito ou rejeitado  \n",
    "        sdf_copia=sdf_edits['Areas'].copy()\n",
    "        sdf_edits['Areas']=sdf_edits['Areas'][sdf_edits['Areas']['erro'].isna()]\n",
    "        sublayers=['Areas']\n",
    "        #sublayers=['Areas_Intersect','Areas_Aprovadas','Areas']\n",
    "\n",
    "        if tipo in actions:\n",
    "            for sublayer in sublayers:\n",
    "                if not sdf_edits[sublayer].empty:\n",
    "                    print(sublayer)\n",
    "                    #Monta o feature set para adicionar no arcgis\n",
    "                    fields = [i['name'] for i in layer_projeto[Projeto][sublayer]['sublayer'].properties.fields if i['editable']]\n",
    "                    if sublayer != 'Areas_Aprovadas':\n",
    "                        fields.append('SHAPE')\n",
    "                    if tipo == 'Edicao':\n",
    "                        fields.append('OBJECTID')\n",
    "                    edit_fs_df = pd.DataFrame(columns=fields)\n",
    "                    for col in edit_fs_df.columns:\n",
    "                        if col in sdf_edits[sublayer]:\n",
    "                            edit_fs_df[col] = sdf_edits[sublayer][col].values\n",
    "                    feature_set = edit_fs_df.spatial.to_featureset()\n",
    "                    display(sdf_edits[sublayer])\n",
    "                    #aplica as acoes necesserias \n",
    "                    action, arg = actions[tipo][sublayer]\n",
    "                    if action == 'updates':\n",
    "                        result = layer_projeto[Projeto][sublayer]['sublayer'].edit_features(updates=feature_set)\n",
    "                    elif action == 'adds':\n",
    "                        efetua_exclusoes(sdf_edits[sublayer], sublayer, arg, Projeto)\n",
    "                        result = layer_projeto[Projeto][sublayer]['sublayer'].edit_features(adds=feature_set)\n",
    "\n",
    "                    print(result)\n",
    "\n",
    "                    if(result):\n",
    "                        if(tipo=='Edicao'):print(\"features editados com sucesso\")\n",
    "                        if(tipo=='Inclusao'):print(\"features incluidos com sucesso\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6082de3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T13:03:02.127305Z",
     "iopub.status.busy": "2023-08-07T13:03:02.126157Z",
     "iopub.status.idle": "2023-08-07T13:03:02.127863Z",
     "shell.execute_reply": "2023-08-07T13:03:02.128174Z"
    },
    "papermill": {
     "duration": 0.045981,
     "end_time": "2023-08-07T13:03:02.128370",
     "exception": false,
     "start_time": "2023-08-07T13:03:02.082389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def encotra_areas_inutilizadas(frase):\n",
    "    import re\n",
    "    padrao = r\"PROP-\\w{3}-\\d{4}\"\n",
    "\n",
    "    ocorrencias = re.findall(padrao, frase)\n",
    "    return ocorrencias\n",
    "\n",
    "def faz_exclusoes():\n",
    "    items=sharepoint_lists_fundiario(ctx,'Solicitar Modificacao de base')\n",
    "    exclusoes=pd.DataFrame(columns=[\"ID_pedido\", \"Projeto\",\"Exclusao\"])\n",
    "    for item in items:\n",
    "        if(item.properties[\"TipodeOperacaonabase\"]=='Exclusao'):\n",
    "            if(item.properties[\"Areaaserexcluida\"] is not None):\n",
    "                try:\n",
    "                    for exclusao in encotra_areas_inutilizadas(item.properties[\"Areaaserexcluida\"]):\n",
    "                        exclusoes=exclusoes.append({ \n",
    "                                        \"ID_pedido\": item.properties[\"Id\"],\n",
    "                                        \"Projeto\":item.properties['Projeto'],\n",
    "                                        \"area_code\":exclusao\n",
    "                                    }, ignore_index=True)\n",
    "                except:\n",
    "                    item.set_property('Title', \"Erro1\")\n",
    "                    item.update()\n",
    "                    ctx.execute_query()\n",
    "                exclusoes['TipodeOperacaonabase']=[item.properties[\"TipodeOperacaonabase\"]] * len(exclusoes)\n",
    "    if not exclusoes.empty:                                                      \n",
    "        efetua_exclusoes(exclusoes,'Areas',True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6de9cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T13:03:02.219848Z",
     "iopub.status.busy": "2023-08-07T13:03:02.219034Z",
     "iopub.status.idle": "2023-08-07T13:03:02.221305Z",
     "shell.execute_reply": "2023-08-07T13:03:02.220934Z"
    },
    "papermill": {
     "duration": 0.057361,
     "end_time": "2023-08-07T13:03:02.221424",
     "exception": false,
     "start_time": "2023-08-07T13:03:02.164063",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from arcgis import GIS\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from arcgis.geometry import Polygon, Geometry, Point, Polyline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def retorna_novo_codigo_GPD(Projeto,sdf,dict_layers):\n",
    "    #### insere os numeros das props quando e INCLUSAO\n",
    "    lista_area_code = dict_layers[Projeto]['Areas']['sdf']['area_code'].unique()\n",
    "    maximo = int(max(lista_area_code, key=lambda x: int(x.split('-')[-1])).split('-')[2])\n",
    "    \n",
    "    if lista_area_code is []: sdf['area_code'] = '0'\n",
    "    \n",
    "    # gerar uma sequência de novos números\n",
    "    n_novos_itens = len(sdf)\n",
    "    novos_numeros = range(maximo+1, maximo+1+n_novos_itens)\n",
    "\n",
    "    # atribuir os novos números à coluna area_code\n",
    "    df_novos = pd.DataFrame({'area_code': [f'PROP-{str(Projeto).zfill(3)}-{str(n).zfill(4)}' for n in novos_numeros]})\n",
    "\n",
    "    #atribui as colunas que nao tao no padrao\n",
    "    sdf['area_code'] = df_novos['area_code'].values\n",
    "    \n",
    "    return sdf\n",
    "\n",
    "def verifica_intersect(sdf,filename):\n",
    "    \n",
    "    #Inicia as variaveis necessarias para rastreio\n",
    "    ID_pedido=filename.split('_')[3][:-4]\n",
    "    ID_author=filename.split('_')[2]\n",
    "    Projeto=filename.split('_')[0]\n",
    "    TipodeOperacaonabase=filename.split('_')[1]\n",
    "    \n",
    "    #Variaveis do pedido\n",
    "    sdf['Projeto']=[Projeto]*len(sdf)\n",
    "    sdf['TipodeOperacaonabase']=[TipodeOperacaonabase]* len(sdf)\n",
    "    \n",
    "    print(\"inicio\")\n",
    "    #sdf de analises das props\n",
    "    dict_layers=retorna_df_areas_projetos([Projeto])\n",
    "    areas=dict_layers[Projeto]['Areas']['sdf']\n",
    "    areas_filtradas = areas[areas['area_code'].str.contains(Projeto)]\n",
    "    \n",
    "    if(TipodeOperacaonabase=='Inclusao'):\n",
    "        sdf=sdf.rename(columns={'Proprieta': 'Proprietario_principal'})\n",
    "    # Um sdf do tipo edicao nao tem as infos dentro do poligono enquanto a inclusao nao tem area_code\n",
    "    if(TipodeOperacaonabase=='Edicao'):\n",
    "        #para a edicao conter todos os campos e capturar o OBJECT ID conrrespondente\n",
    "        if 'OBJECTID' in sdf.columns:sdf = sdf.drop('OBJECTID', axis=1)\n",
    "        sdf = pd.merge(sdf, areas[['area_code','OBJECTID','Status',\"Proprietario_principal\",'Contrato',\"Matricula\"]], on='area_code')\n",
    "        #eliminando as areas com o mesmo area_code da area a ser editada no projeto , eliminando area de interseccao com ela mesma\n",
    "        area_codes = sdf['area_code'].unique()\n",
    "        mask = areas_filtradas['area_code'].isin(area_codes)\n",
    "        areas_filtradas.drop(areas_filtradas[mask].index, inplace=True)\n",
    "    \n",
    "    #renomeando para nao haver campo \"area_code\" duplicados\n",
    "    areas_filtradas=areas_filtradas.rename(columns={'area_code': 'area_code_intersect'})\n",
    "    sdf_sem_erros=sdf[sdf['erro'].isna()]\n",
    "    sdf_sem_erros.spatial.set_geometry('SHAPE', inplace=True)\n",
    "    sdf_intersect_props=encontra_sobreposicao(sdf_sem_erros, areas_filtradas,'OBJECTID','area_code_intersect')\n",
    "    sdf_intersect_props=sdf_intersect_props.rename(columns={'area_acres': 'area_acres_intersect'})\n",
    "    display(sdf_intersect_props)\n",
    "    if not(sdf_intersect_props.empty):\n",
    "        #Aplica a condicao de aceitacao e recusa do poligono onde se houver mais de 2(ha) de interseccao a area sera recusada e enviada para revisao (7)\n",
    "        merge_condition=sdf_intersect_props['area_acres_intersect']>2\n",
    "        aggregated_df = sdf_intersect_props[merge_condition].drop_duplicates(subset='OBJECTID')\n",
    "        sdf = sdf.merge(aggregated_df[['area_acres_intersect','area_code_intersect','OBJECTID']], on='OBJECTID', how='left')\n",
    "        display(sdf)\n",
    "        condicao = sdf['area_acres_intersect'] > 2\n",
    "        sdf = atribuir_erro(sdf, 7, condicao)\n",
    "\n",
    "    #Dicionario que contem os edits do sublayer , ira-se adicionar apenas os que nao contem erros \n",
    "    sdf_operacoes_mapas={'Areas':sdf,'Areas_Intersect': sdf_intersect_props, 'Areas_Aprovadas': pd.DataFrame()}    \n",
    "    \n",
    "    if sdf[sdf['erro'].isna()].empty:\n",
    "        print(\"O SpatialDataFrame está vazio ,todas as props foram recusadas devido a interseccoes maiores que 2(ha) para o id\",ID_pedido)\n",
    "    else:\n",
    "        #Agora verifica se a area intersecta buffers de linhas de aeros ou poligonos de paineis solares \n",
    "        print(\"O DataFrame não está vazio.\")\n",
    "        \n",
    "        if(TipodeOperacaonabase=='Edicao'):\n",
    "            sdf['ID_Editor']=[ID_author]* len(sdf)\n",
    "        if(TipodeOperacaonabase=='Inclusao'):    \n",
    "            sdf['ID_Creator']=[ID_author]* len(sdf)\n",
    "            sdf.loc[sdf['erro'].isna(),'area_code']=retorna_novo_codigo_GPD(Projeto,sdf[sdf['erro'].isna()],dict_layers)\n",
    "            sdf_intersect_props=sdf_intersect_props.merge(sdf[['OBJECTID', 'area_code']], on='OBJECTID', how='left')\n",
    "            sdf_intersect_props = sdf_intersect_props.drop('OBJECTID', axis=1)\n",
    "            sdf = sdf.drop('OBJECTID', axis=1)\n",
    "        inicio=True\n",
    "        if Projeto in projetos_GPD:\n",
    "            for buffer in ['Linhas_EOL', 'Linhas_UFV']:\n",
    "                linhas_aeros = dict_layers[Projeto][buffer]['sdf']\n",
    "                if not linhas_aeros.empty:\n",
    "                    linhas_filtradas = linhas_aeros[linhas_aeros['linha_code'].str.contains(Projeto)]\n",
    "                    \n",
    "                    sdf_sem_erros = sdf[sdf['erro'].isna()]\n",
    "                    sdf_sem_erros.spatial.set_geometry('SHAPE', inplace=True)\n",
    "                    sdf_linha_props = encontra_sobreposicao(sdf_sem_erros, linhas_filtradas, 'area_code', 'linha_code')\n",
    "                    area_linha=sum([retorna_area(area_linha) for area_linha in linhas_filtradas[\"SHAPE\"]])\n",
    "                    sdf_linha_props['area_acres']=100*sdf_linha_props['area_acres']/area_linha\n",
    "                    \n",
    "                    if(inicio):\n",
    "                        sdf_linhas=sdf_linha_props\n",
    "                        inicio=False\n",
    "                    else:\n",
    "                        if not (sdf_linha_props.empty):\n",
    "                            sdf_linhas = pd.concat([sdf_linha_props, sdf_linhas], ignore_index=True)\n",
    "            \n",
    "            sdf['Status_Aprovacao'] = [\"Parcialmente Aprovado\"] * len(sdf)\n",
    "            if(inicio):\n",
    "                #sdf.loc[sdf['area_code'].isin(sdf_linhas['area_code']), 'linha_code'] = sdf_linha_props['linha_code']\n",
    "                sdf.loc[sdf['area_code'].isin(sdf_linhas['area_code']), 'Status_Aprovacao'] = 'Aprovado'\n",
    "                sdf_operacoes_mapas['Areas_Aprovadas'] = sdf_linhas\n",
    "                sdf_operacoes_mapas['Areas'] = sdf\n",
    "\n",
    "                print(\"fim\")\n",
    "        \n",
    "    print(\"sdf do pedido\")\n",
    "    display(sdf_operacoes_mapas['Areas'])        \n",
    "    #Adiciona o sdf e muda o status do pedido pra Aceito \n",
    "    edit_features_sdf(sdf_operacoes_mapas,TipodeOperacaonabase,dict_layers,Projeto)\n",
    "\n",
    "    return sdf\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f3f171",
   "metadata": {
    "papermill": {
     "duration": 0.03563,
     "end_time": "2023-08-07T13:03:02.292574",
     "exception": false,
     "start_time": "2023-08-07T13:03:02.256944",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Executa o processo de verificacoes dos shps e atualizacoes nos mapas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c35f9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T13:03:02.376426Z",
     "iopub.status.busy": "2023-08-07T13:03:02.375600Z",
     "iopub.status.idle": "2023-08-07T13:03:42.388924Z",
     "shell.execute_reply": "2023-08-07T13:03:42.389230Z"
    },
    "papermill": {
     "duration": 40.058336,
     "end_time": "2023-08-07T13:03:42.389389",
     "exception": false,
     "start_time": "2023-08-07T13:03:02.331053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def separar_por_props_novas(sdf):\n",
    "    sdf_com_objectid = sdf[sdf['TipodeOperacaonabase']=='Edicao']\n",
    "    sdf_sem_objectid = sdf[sdf['TipodeOperacaonabase']=='Inclusao']\n",
    "    return sdf_com_objectid, sdf_sem_objectid\n",
    "\n",
    "def main():\n",
    "    caminho='/arcgis/home/Nao Incluidos'\n",
    "    inicio=True\n",
    "    \n",
    "    # percorre o diretório e realiza as operações necessárias\n",
    "    for root, dirs, files in os.walk(caminho):\n",
    "        for diretorio in dirs:\n",
    "            if(diretorio in ['Inclusao','Edicao','Divisao']):\n",
    "                print(\"No diretório: \", diretorio)\n",
    "                diretorio_atual = os.path.join(root, diretorio)\n",
    "                for file in os.listdir(diretorio_atual):\n",
    "                    if file.endswith(\".shp\"):\n",
    "                        path=os.path.join(root,diretorio_atual,file)\n",
    "                        print(\"Coletando info geograficas do .shp\", path)\n",
    "                        sdf=verifica_campos_vazios(sdf_from_shp(path),diretorio,file)\n",
    "                        sdf=verifica_intersect(sdf,file)\n",
    "                        confirma_pedidos(sdf,diretorio_atual)\n",
    "                        if(inicio):\n",
    "                            sdf_pedidos=sdf\n",
    "                            inicio=False\n",
    "                        else:\n",
    "                            sdf_pedidos = pd.concat([sdf, sdf_pedidos], ignore_index=True)\n",
    "                                                              \n",
    "    if not sdf_pedidos.empty:\n",
    "        print(\"Informacoes coletadas\")\n",
    "        sdf_pedidos.rename(columns={'Proprieta': 'Proprietario_principal'}, inplace=True)\n",
    "        print(\"Os pedidos aceitos foram:\")\n",
    "        display(sdf_pedidos)\n",
    "        #sdf_adds=separar_por_props_novas(sdf_pedidos)\n",
    "        colunas=['Projeto','area_code','Proprietario_principal','Imovel','Status','Matricula','Contrato']\n",
    "        print(\"Os pedidos recusados foram:\")\n",
    "        display(sdf_pedidos.loc[~sdf_pedidos['erro'].isna()])\n",
    "    else:\n",
    "        print(\"So ha pedidos recusados\")\n",
    "        display(sdf_pedidos.loc[~sdf_pedidos['erro'].isna()])\n",
    "\n",
    "#faz_exclusoes()\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92f7d8c",
   "metadata": {
    "papermill": {
     "duration": 0.056139,
     "end_time": "2023-08-07T13:03:42.502129",
     "exception": false,
     "start_time": "2023-08-07T13:03:42.445990",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 03-Saidas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba12fe44",
   "metadata": {
    "papermill": {
     "duration": 0.064766,
     "end_time": "2023-08-07T13:03:42.623213",
     "exception": false,
     "start_time": "2023-08-07T13:03:42.558447",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Captura as inclusoes recentes e insre numa planilha com uma macro que insere nas planilhas GPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407caa15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T13:03:42.744208Z",
     "iopub.status.busy": "2023-08-07T13:03:42.736501Z",
     "iopub.status.idle": "2023-08-07T13:03:42.746191Z",
     "shell.execute_reply": "2023-08-07T13:03:42.745843Z"
    },
    "papermill": {
     "duration": 0.067692,
     "end_time": "2023-08-07T13:03:42.746306",
     "exception": false,
     "start_time": "2023-08-07T13:03:42.678614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def retorna_projeto_data(titulo_projeto,mes,dia,ano):\n",
    "    map_projeto = gis.content.search('title:'+titulo_projeto,'Feature Layer')\n",
    "    mapa = map_projeto[retorna_posicao(map_projeto,titulo_projeto)]\n",
    "    Projeto_layers= mapa.layers\n",
    "    for lyr in Projeto_layers:\n",
    "            if(lyr.properties.name == 'Areas'):\n",
    "                    layer=lyr\n",
    "    data_alvo = datetime.date(ano, mes, dia) # define a data de referência\n",
    "    #ex:data_alvo = datetime.date(2023, 3, 12)\n",
    "    data_alvo_str = data_alvo.isoformat() # converte para uma string no formato 'AAAA-MM-DD'\n",
    "    filtro_data = f\"CreationDate >= TIMESTAMP '{data_alvo_str}'\"\n",
    "    resultados = layer.query(where=filtro_data)\n",
    "    areas=resultados.sdf\n",
    "    return areas\n",
    "def captura_inclusoes_hoje():\n",
    "    projetos_atualizados=[]\n",
    "    # Obter a data atual\n",
    "    data_atual = datetime.date.today()\n",
    "\n",
    "    # Obter o dia\n",
    "    dia = data_atual.day\n",
    "    #dia=8\n",
    "\n",
    "    # Obter o mês\n",
    "    mes = data_atual.month\n",
    "    #mes=6\n",
    "\n",
    "    inicio=True\n",
    "    sdf_pedidos=pd.DataFrame()\n",
    "    for projeto in projetos:\n",
    "        print(projeto)\n",
    "        sdf=retorna_projeto_data(projeto,mes,dia,2023)\n",
    "        if not sdf.empty:\n",
    "            print(\"Ha itens novos para o projeto\",projeto)\n",
    "            projetos_atualizados.append(projeto)\n",
    "            sdf['Projeto']=[projeto]*len(sdf)\n",
    "            if(inicio):\n",
    "                sdf_pedidos=sdf\n",
    "                inicio=False\n",
    "            else:\n",
    "                sdf_pedidos=pd.concat([sdf_pedidos, sdf], ignore_index=True)\n",
    "    if not sdf_pedidos.empty:\n",
    "        heading_meses=[\"Situação do imóvel jan/23\",\"Situação do imóvel fev/23\", \"Situação do imóvel mar/23\", \"Situação do imóvel abr/23\",\n",
    "                          \"Situação do imóvel mai/23\",\"Situação do imóvel jun/23\", \"Situação do imóvel jul/23\", \"Situação do imóvel ago/23\",\n",
    "                          \"Situação do imóvel set/23\",\"Situação do imóvel out/23\", \"Situação do imóvel nov/23\", \"Situação do imóvel dez/23\"]            \n",
    "\n",
    "\n",
    "        colunas=['Projeto','area_code','Proprietario_principal','Imovel','Status','Matricula','Contrato']\n",
    "        df=sdf_pedidos[colunas]\n",
    "\n",
    "        df = df.rename(columns={'Proprietario_principal': 'Proprietário principal'})\n",
    "        df = df.rename(columns={'area_code': 'AREA_CODE'})\n",
    "        df = df.rename(columns={'Imovel': 'Imóvel'})\n",
    "        df['mes']=len(df)*[heading_meses[mes-1]]\n",
    "\n",
    "        return (df,projetos_atualizados)\n",
    "    \n",
    "    else: return (sdf,projetos_atualizados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06404f4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T13:03:42.861295Z",
     "iopub.status.busy": "2023-08-07T13:03:42.860568Z",
     "iopub.status.idle": "2023-08-07T13:03:42.862149Z",
     "shell.execute_reply": "2023-08-07T13:03:42.862496Z"
    },
    "papermill": {
     "duration": 0.061168,
     "end_time": "2023-08-07T13:03:42.862632",
     "exception": false,
     "start_time": "2023-08-07T13:03:42.801464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = '/arcgis/home/modificacoes na base.xlsm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470b46ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T13:03:42.985153Z",
     "iopub.status.busy": "2023-08-07T13:03:42.984318Z",
     "iopub.status.idle": "2023-08-07T13:03:43.266227Z",
     "shell.execute_reply": "2023-08-07T13:03:43.265819Z"
    },
    "papermill": {
     "duration": 0.348043,
     "end_time": "2023-08-07T13:03:43.266355",
     "exception": false,
     "start_time": "2023-08-07T13:03:42.918312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "\n",
    "arquivo_adds=dataset\n",
    "\n",
    "def salva_adds_excel(df):\n",
    "    # Carregar o arquivo Excel existente\n",
    "    workbook = openpyxl.load_workbook(arquivo_adds,keep_vba=True)\n",
    "\n",
    "    # Selecionar a planilha \"Indice\"\n",
    "    sheet = workbook['Indice']\n",
    "\n",
    "    # Criar um dicionário para mapear o nome da coluna para o número da coluna na planilha \"Indice\"\n",
    "    column_dict = {}\n",
    "    for column in sheet.iter_cols(min_row=1, max_row=1):\n",
    "        for cell in column:\n",
    "            column_dict[cell.value] = cell.column\n",
    "\n",
    "    # Adicionar uma coluna com os índices do DataFrame original\n",
    "    df['Indice'] = df.index\n",
    "\n",
    "    # Agrupar as linhas por projeto no dataframe\n",
    "    agrupado = df.groupby('Projeto')\n",
    "\n",
    "    # Apagar os valores da coluna \"Importado fundiário\"\n",
    "    for row in sheet.iter_rows(min_row=2, min_col=column_dict['Importado fundiário'], max_col=10):\n",
    "        row[0].value = None\n",
    "\n",
    "    for row in sheet.iter_rows(min_row=2, min_col=column_dict['Linha inicial'], max_col=10):\n",
    "        row[0].value = None\n",
    "\n",
    "    for row in sheet.iter_rows(min_row=2, min_col=column_dict['Linha final'], max_col=10):\n",
    "        row[0].value = None    \n",
    "\n",
    "    # Percorrer os grupos de projetos\n",
    "    for projeto, grupo in agrupado:\n",
    "        # Localizar a linha correspondente na coluna \"Projeto\" da planilha \"Indice\"\n",
    "        for row in sheet.iter_rows(min_row=2, max_col=column_dict['Projeto'], max_row=sheet.max_row):\n",
    "            if row[0].value == projeto:\n",
    "                # Obter a primeira e última linha do grupo no dataframe original\n",
    "                linha_inicial = grupo['Indice'].iloc[0] + 2  # Adiciona 2 para considerar o cabeçalho e a indexação baseada em 1 do Excel\n",
    "                linha_final = grupo['Indice'].iloc[-1] + 2\n",
    "\n",
    "                # Atualizar as colunas \"Linha inicial\" e \"Linha final\" na planilha \"Indice\"\n",
    "                sheet.cell(row=row[0].row, column=column_dict['Linha inicial']).value = linha_inicial\n",
    "                sheet.cell(row=row[0].row, column=column_dict['Linha final']).value = linha_final\n",
    "\n",
    "                # Atualizar a coluna \"Importado\" com \"Ok\"\n",
    "                sheet.cell(row=row[0].row, column=column_dict['Importado fundiário']).value = 'Ok'\n",
    "\n",
    "                break\n",
    "\n",
    "\n",
    "    ws = workbook['CSV']\n",
    "\n",
    "    # Limpar a aba \"CSV\" antes de inserir os novos valores\n",
    "    ws.delete_rows(2, ws.max_row)\n",
    "\n",
    "    # Criar um dicionário para mapear o nome da coluna para o número da coluna na planilha \"Indice\"\n",
    "    column_dict = {}\n",
    "    for column in ws.iter_cols(min_row=1, max_row=1):\n",
    "        for cell in column:\n",
    "            column_dict[cell.value] = cell.column\n",
    "\n",
    "    # Inserir os valores na aba \"CSV\"\n",
    "    for r_idx, row in enumerate(df.values, 2):\n",
    "        for col_name, value in zip(df.columns, row):\n",
    "            if col_name in column_dict:\n",
    "                col_idx = column_dict[col_name]\n",
    "                ws.cell(row=r_idx, column=col_idx, value=value)\n",
    "\n",
    "\n",
    "\n",
    "    # Salvar as alterações no arquivo Excel\n",
    "    workbook.save(arquivo_adds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ba628f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T13:03:43.383666Z",
     "iopub.status.busy": "2023-08-07T13:03:43.382648Z",
     "iopub.status.idle": "2023-08-07T13:04:25.641264Z",
     "shell.execute_reply": "2023-08-07T13:04:25.640818Z"
    },
    "papermill": {
     "duration": 42.319625,
     "end_time": "2023-08-07T13:04:25.641389",
     "exception": false,
     "start_time": "2023-08-07T13:03:43.321764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "(df,projetos_atualizados)=captura_inclusoes_hoje()\n",
    "if not df.empty:\n",
    "    salva_adds_excel(df)\n",
    "else : print(\"nao ha areas novas adicionadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c223970e",
   "metadata": {
    "papermill": {
     "duration": 0.061409,
     "end_time": "2023-08-07T13:04:25.764273",
     "exception": false,
     "start_time": "2023-08-07T13:04:25.702864",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Gera os .shp e os kmls atualizados para o sist. fundiario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ec9a2c",
   "metadata": {
    "papermill": {
     "duration": 0.060693,
     "end_time": "2023-08-07T13:04:25.885554",
     "exception": false,
     "start_time": "2023-08-07T13:04:25.824861",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Cria o kmz e shp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ee2028",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T13:04:26.015060Z",
     "iopub.status.busy": "2023-08-07T13:04:26.013673Z",
     "iopub.status.idle": "2023-08-07T13:04:26.015546Z",
     "shell.execute_reply": "2023-08-07T13:04:26.015855Z"
    },
    "papermill": {
     "duration": 0.06879,
     "end_time": "2023-08-07T13:04:26.016000",
     "exception": false,
     "start_time": "2023-08-07T13:04:25.947210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from arcgis.features import GeoAccessor\n",
    "\n",
    "def run_notebook(notebookParameters,nb_id):\n",
    "    nb_server = gis.notebook_server[0]\n",
    "    notebook_item = gis.content.get(nb_id)\n",
    "    nb_mgr = nb_server.notebooks\n",
    "    nb_result = nb_mgr.execute_notebook(notebook_item,update_portal_item=True,parameters=notebookParameters,save_parameters=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68857386",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T13:04:26.144599Z",
     "iopub.status.busy": "2023-08-07T13:04:26.143836Z",
     "iopub.status.idle": "2023-08-07T13:04:26.145947Z",
     "shell.execute_reply": "2023-08-07T13:04:26.145592Z"
    },
    "papermill": {
     "duration": 0.069056,
     "end_time": "2023-08-07T13:04:26.146054",
     "exception": false,
     "start_time": "2023-08-07T13:04:26.076998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "notebooks={'cria_kml_shp':\n",
    " {'id':'','notebookParameters':{\"tipo\":'Status Parcial',\"projetos\":projetos_atualizados,\"gera_shp\":True}},\n",
    " 'Upload_fundiario':{'id':'','notebookParameters':{\"action_sharepoint\": 'Upload_items','pasta_sharepoint':'Fundiario',\"delete_file\":False}},\n",
    " 'Upload_coleta':{'id':'','notebookParameters':{\"action_sharepoint\": 'Upload_items','pasta_sharepoint':'Coleta',\"delete_file\":True}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8469096",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T13:04:26.273231Z",
     "iopub.status.busy": "2023-08-07T13:04:26.272780Z",
     "iopub.status.idle": "2023-08-07T13:19:28.280379Z",
     "shell.execute_reply": "2023-08-07T13:19:28.279954Z"
    },
    "papermill": {
     "duration": 902.073521,
     "end_time": "2023-08-07T13:19:28.280514",
     "exception": false,
     "start_time": "2023-08-07T13:04:26.206993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for notebook in notebooks.keys():\n",
    "    run_notebook(notebooks[notebook][\"notebookParameters\"],notebooks[notebook][\"id\"])\n",
    "    time.sleep(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79657820",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "esriNotebookRuntime": {
   "notebookRuntimeName": "ArcGIS Notebook Python 3 Standard",
   "notebookRuntimeVersion": "7.0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1008.719463,
   "end_time": "2023-08-07T13:19:28.975761",
   "environment_variables": {},
   "exception": null,
   "input_path": "/arcgis/home/.tasks/6cc1180434db4f0c848932bb453a1a67/25aafab0d51d492fa2127d63891734cd.ipynb",
   "output_path": "/arcgis/home/.tasks/6cc1180434db4f0c848932bb453a1a67/output.ipynb",
   "start_time": "2023-08-07T13:02:40.256298",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
